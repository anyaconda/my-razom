{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 1/10/2024 NLP for UDS. Part 1. Data Prep\n",
    "# Text Analytics. Trends and topic modeling for UDS\n",
    "#      Data: How did you...\n",
    "#      Task: Explore data before modeling\n",
    "\n",
    "#      input: data/xx_howdidu_tidy.csv\n",
    "      \n",
    "\n",
    "#started from nlp_2_model.ipynb \n",
    "#need to explore data before deciding how to prep data for ML\n",
    "\n",
    "\n",
    "#history\n",
    "#1/10/2024 EXPLORE DATA  $ac\n",
    "\n",
    "\n",
    "#Pipeline: nlp_0_data -> nlp_1_dataprep (here) -> nlp_2_model \n",
    "\n",
    "#$config $manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "#from joblib import load, dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k3/_k06_ggs0jx1bf4h7lpgh00r0000gn/T/ipykernel_913/3174619232.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Import the wordcloud library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# #plotly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# dataprep - tokenization\n",
    "from sklearn.feature_extraction import text\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer #transformer to tokenize dataset, aka bag-of-words activity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #rescale features by how informative they are\n",
    "\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# #plotly\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.version)\n",
    "np.__version__, pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global vars and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "DATA_DIR = 'data'\n",
    "DATA_FILE_IN = DATA_DIR + '/howdidu_tidy.csv' #$config\n",
    "\n",
    "#IF\n",
    "FLAG_SUBSET = False #$config for initial confirming smaller counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Text Analytics \n",
    "Data preparation for Text Analytics \n",
    "\n",
    "## 0. Load Tidy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidy = pd.read_csv(DATA_FILE_IN)\n",
    "\n",
    "if FLAG_SUBSET:\n",
    "    df_tidy = df_tidy[:20].copy() #$temp\n",
    "\n",
    "print(df_tidy.shape)\n",
    "print(df_tidy.columns)\n",
    "df_tidy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_tidy[df_tidy['Contact rpt. description'].str.contains('Passcode')] #other strings: '12532050468', '\\'', '\\$', 'donat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_tidy[df_tidy['Contact rpt. description'].str.contains('ukrainedefensesupport')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prep Data for ML\n",
    "Use `Contact rpt. description` for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "#feature engineering - concatentate text columns? here, no\n",
    "df_tidy['all_text'] = df_tidy['Contact rpt. description'].str.replace('\\s',' ', regex=True) #\\s stands for “whitespace character”. In all flavors, it includes [ \\t\\r\\n\\f]\n",
    "df_tidy['all_text'] = df_tidy['all_text'].str.lower()\n",
    "#$manual rules\n",
    "df_tidy['all_text'] = df_tidy['all_text'].str.replace('[\\’\\']s\\s',' ', regex=True)\n",
    "df_tidy['all_text'] = df_tidy['all_text'].str.replace('i[\\’\\']m\\s','im ', regex=True)\n",
    "df_tidy['all_text'] = df_tidy['all_text'].str.replace('y[\\’\\']all','you all', regex=True)\n",
    "df_tidy['all_text'] = df_tidy['all_text'].str.replace('n/a','__na__') #replace n/a\n",
    "\n",
    "print(df_tidy.shape)\n",
    "df_tidy.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prepare for Text Analytics \n",
    "\n",
    "Assemble Data into ML Expected Format.  \n",
    "Scikit-learn expects a Numpy array-like structure. Transform to a structure acceptable by algorithm: \n",
    "- input features X(matrix) aka `train set`  \n",
    "- target variable y(vector) \n",
    "\n",
    "Here:  \n",
    "- X - column 'all_text'  = `train set`  \n",
    "- y - n/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interested in words in all rows, column 0\n",
    "text_train = df_tidy['all_text'] #.iloc[:,0]\n",
    "text_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Verify Text and Counts\n",
    "by comparing with `Collections.Counter` and `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join \n",
    "long_string = ' '.join(list(text_train)) #keep case\n",
    "len(long_string), long_string[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2a `Counter()`\n",
    "dict subclass for counting hashable objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# split on white-space: good but has problems with extra punctuation\n",
    "# l_long_string = re.split('[,\\s\\.]+', long_string) #list\n",
    "# long_string_tidy = ' '.join(l_long_string)\n",
    "\n",
    "#split words followed by space only: good but has problems with missing numbers\n",
    "# long_string_tidy = re.sub(\"[^\\w ]\", \"\", long_string)\n",
    "\n",
    "#split [words / numbers / _] followed by space\n",
    "long_string_tidy = re.sub(\"[^a-zA-Z0-9_\\$]\", \" \", long_string)\n",
    "\n",
    "long_string_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count word frequency & sort \n",
    "tokens = long_string_tidy.lower().split()\n",
    "cc_word_counter = Counter(tokens) #class collections.Counter\n",
    "\n",
    "#preview\n",
    "print(\"cc_word_counter: \", len(cc_word_counter))\n",
    "print(\"View cc_word_counter: \", cc_word_counter)\n",
    "# list unique elements\n",
    "#list(cc_word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm counts\n",
    "#  w/o Counter, acting like it's a dict datastruct\n",
    "print(\"Unique tokens: {}, Total count: {}\".format(len(cc_word_counter),cc_word_counter.total()))\n",
    "#  w Counter, seems convoluted\n",
    "print(\"Unique tokens: {}, Total count: {}\".format(Counter(cc_word_counter.values()).total(), Counter(dict(cc_word_counter)).total()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm counts w/ N most common\n",
    "N_MOST = 10\n",
    "cc_word_counter_10 = cc_word_counter.most_common(N_MOST) #class list\n",
    "print(cc_word_counter_10)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Unique tokens: {}, Total count: {}\".format(len(cc_word_counter_10), sum(dict(cc_word_counter_10).values())))\n",
    "print(\"Unique tokens: {}, Total count: {}\".format(Counter(cc_word_counter_10).total(), Counter(dict(cc_word_counter_10)).total()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N least common\n",
    "cc_word_counter.most_common()[:-N_MOST-1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2b `CountVectorizer()`\n",
    "Convert a long string to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv =  CountVectorizer(token_pattern='[a-zA-Z0-9_\\$]+') #$config lcase lowercase=True, \n",
    "# 1) tokenize train data and build the vocabulary + 2)`transform` converts text to a bow representation: SciPy sparse matrix only stores non-zero entries\n",
    "cv_fit = cv.fit_transform(text_train) #class scipy.sparse._csr.csr_matrix\n",
    "cv_vocab = cv.get_feature_names_out() #class numpy.ndarray\n",
    "print(\"Vocab: \", len(cv_vocab))\n",
    "print(\"Preview vocab: \", cv_vocab[:10])\n",
    "\n",
    "cv_fit.toarray().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup token\n",
    "cv_count_list = cv_fit.toarray().sum(axis=0)\n",
    "\n",
    "d_all = dict(zip(cv_vocab,cv_count_list))\n",
    "\n",
    "token = 'Carl'\n",
    "try:\n",
    "    this_token = token\n",
    "    this_count = d_all[this_token]\n",
    "except KeyError:\n",
    "    this_token = token.lower()\n",
    "    this_count = d_all[this_token]\n",
    "finally:\n",
    "    print(this_token, this_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_vocab, cv_count_list, cv_count_list.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(cc_word_counter.values()) == cv_count_list.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2c `TfidfVectorizer()`\n",
    "\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features  \n",
    "Equivalent to `CountVectorizer` followed by `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v = TfidfVectorizer(token_pattern='[a-zA-Z0-9_\\$]+')#$config lcase lowercase=True, \n",
    "# 1) tokenize train data and build the vocabulary + 2)`transform` converts text to a bow representation: SciPy sparse matrix only stores non-zero entries\n",
    "tfidf_v_fit = tfidf_v.fit_transform(text_train) #class scipy.sparse._csr.csr_matrix\n",
    "tfidf_v_vocab = tfidf_v.get_feature_names_out() #class numpy.ndarray\n",
    "print(\"Vocab: \", len(tfidf_v_vocab))\n",
    "print(\"Preview vocab: \", tfidf_v_vocab[:10])\n",
    "\n",
    "tfidf_v_fit.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v_count_list = tfidf_v_fit.toarray().sum(axis=0)\n",
    "tfidf_v_vocab, tfidf_v_count_list, tfidf_v_count_list.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Diff between CC, CV and TfidfV vocabs\n",
    "eventually get to the point of no diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff\n",
    "#list(cc_word_counter), cv_vocab.tolist()\n",
    "\n",
    "np.setdiff1d(list(cc_word_counter), cv_vocab.tolist()), np.setdiff1d(cv_vocab.tolist(), list(cc_word_counter)), np.setdiff1d(tfidf_v_vocab.tolist(), list(cc_word_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save 3 vocabs\n",
    "try:\n",
    "    with open('data/my_cc_vocab.csv', 'w', newline='', encoding='UTF-8') as myfile:\n",
    "        wr = csv.writer(myfile, delimiter = '|')\n",
    "        wr.writerows([r] for r in cc_word_counter)\n",
    "except:\n",
    "    print(\"An error occurred\")\n",
    "    for e in sys.exc_info():\n",
    "        print(\"Error details: {}\".format(str(e)))\n",
    "        \n",
    "try:\n",
    "    with open('data/my_cv_vocab.csv', 'w', newline='', encoding='UTF-8') as myfile:\n",
    "        wr = csv.writer(myfile, delimiter = '|')\n",
    "        wr.writerows([r] for r in cv_vocab)\n",
    "except:\n",
    "    print(\"An error occurred\")\n",
    "    for e in sys.exc_info():\n",
    "        print(\"Error details: {}\".format(str(e)))\n",
    "\n",
    "try:\n",
    "    with open('data/my_tfidf_v_vocab.csv', 'w', newline='', encoding='UTF-8') as myfile:\n",
    "        wr = csv.writer(myfile, delimiter = '|')\n",
    "        wr.writerows([r] for r in tfidf_v_vocab)\n",
    "except:\n",
    "    print(\"An error occurred\")\n",
    "    for e in sys.exc_info():\n",
    "        print(\"Error details: {}\".format(str(e)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EDA - with WordCloud\n",
    "src https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud_raw = WordCloud(background_color=\"white\", max_words=5000, include_numbers=True, collocation_threshold=5, width=600, height=300, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud_raw.generate(long_string_tidy)\n",
    "# Visualize the word cloud\n",
    "wordcloud_raw.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Real Vectorizing\n",
    "\n",
    "### 1.3a `CountVectorizer()`\n",
    "with trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 =  CountVectorizer(token_pattern='[a-zA-Z0-9_\\$]+', ngram_range=(1,3)) \n",
    "cv2_fit = cv2.fit_transform(text_train) #class scipy.sparse._csr.csr_matrix\n",
    "cv2_vocab = cv2.get_feature_names_out() #class numpy.ndarray\n",
    "print(\"Vocab: \", len(cv2_vocab))\n",
    "print(\"Preview vocab: \", cv2_vocab[:10])\n",
    "\n",
    "cv2_fit.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove `stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv3 =  CountVectorizer(token_pattern='[a-zA-Z0-9_\\$]+', ngram_range=(1,3), stop_words='english') \n",
    "cv3_fit = cv3.fit_transform(text_train) #class scipy.sparse._csr.csr_matrix\n",
    "cv3_vocab = cv3.get_feature_names_out() #class numpy.ndarray\n",
    "print(\"Vocab: \", len(cv3_vocab))\n",
    "print(\"Preview vocab: \", cv3_vocab[:10])\n",
    "#print(\"cv3_fit:\\n{}\".format(repr(cv3_fit)))\n",
    "\n",
    "cv3_fit.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Diff between CVs trigrams  \n",
    "removed `stopwords` vs not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview diff\n",
    "np.setdiff1d(cv3_vocab, cv2_vocab), np.setdiff1d(cv2_vocab, cv3_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2_count_list = cv2_fit.toarray().sum(axis=0)\n",
    "cv2_vocab, cv2_count_list, cv2_count_list.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv3_count_list = cv3_fit.toarray().sum(axis=0)\n",
    "cv3_vocab, cv3_count_list, cv3_count_list.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3b `TfidfVectorizer()`\n",
    "\n",
    "with trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v2 = TfidfVectorizer(token_pattern='[a-zA-Z0-9_\\$]+', ngram_range=(1,3))\n",
    "tfidf_v2_fit = tfidf_v2.fit_transform(text_train)\n",
    "tfidf_v2_vocab = tfidf_v2.get_feature_names_out() #class numpy.ndarray\n",
    "print(\"Vocab: \", len(tfidf_v2_vocab))\n",
    "print(\"Preview vocab: \", tfidf_v2_vocab[:10])\n",
    "\n",
    "tfidf_v2_fit.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm same n-grams\n",
    "assert len(cv2_vocab) == len(tfidf_v2_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove `stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v3 = TfidfVectorizer(token_pattern='[a-zA-Z0-9_\\$]+', ngram_range=(1, 3), stop_words=\"english\")\n",
    "tfidf_v3_fit = tfidf_v3.fit_transform(text_train)\n",
    "tfidf_v3_vocab = tfidf_v3.get_feature_names_out() #class numpy.ndarray\n",
    "print(\"Vocab: \", len(tfidf_v3_vocab))\n",
    "print(\"Preview vocab: \", tfidf_v3_vocab[:10])\n",
    "\n",
    "tfidf_v3_fit.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm same n-grams\n",
    "assert len(cv3_vocab) == len(tfidf_v3_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Diff between TfidfVs trigrams  \n",
    "removed `stopwords` vs not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview diff\n",
    "np.setdiff1d(tfidf_v3_vocab, tfidf_v2_vocab), np.setdiff1d(tfidf_v2_vocab, tfidf_v3_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v2_count_list = tfidf_v2_fit.toarray().sum(axis=0)\n",
    "tfidf_v2_vocab, tfidf_v2_count_list, tfidf_v2_count_list.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v3_count_list = tfidf_v3_fit.toarray().sum(axis=0)\n",
    "tfidf_v3_vocab, tfidf_v3_count_list, tfidf_v3_count_list.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with `min_df` or `max_df`?   \n",
    "'min_df' gets rid of important tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v4 = TfidfVectorizer(token_pattern='[a-zA-Z0-9_\\$]+', ngram_range=(1, 3), stop_words=\"english\", max_df = .1)\n",
    "tfidf_v4_fit = tfidf_v4.fit_transform(text_train)\n",
    "tfidf_v4_vocab = tfidf_v4.get_feature_names_out() #class numpy.ndarray\n",
    "print(\"Vocab: \", len(tfidf_v4_vocab))\n",
    "print(\"Preview vocab: \", tfidf_v4_vocab[:10])\n",
    "\n",
    "tfidf_v4_fit.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v4_count_list = tfidf_v4_fit.toarray().sum(axis=0)\n",
    "tfidf_v4_vocab, tfidf_v4_count_list, tfidf_v4_count_list.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Diff between TfidfVs trigrams  \n",
    "with `max_df` vs not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview diff\n",
    "np.setdiff1d(tfidf_v4_vocab, tfidf_v3_vocab), np.setdiff1d(tfidf_v3_vocab, tfidf_v4_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uds_stopwords = np.setdiff1d(tfidf_v3_vocab, tfidf_v4_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final vocab\n",
    "try:\n",
    "    with open('data/my_vocab.csv', 'w', newline='', encoding='UTF-8') as myfile:\n",
    "        wr = csv.writer(myfile, delimiter = '|')\n",
    "        wr.writerows([r] for r in tfidf_v4_vocab)\n",
    "except:\n",
    "    print(\"An error occurred\")\n",
    "    for e in sys.exc_info():\n",
    "        print(\"Error details: {}\".format(str(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EDA Final Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find min and max features over dataset:\n",
    "v_min = tfidf_v4_count_list.min()\n",
    "v_max = tfidf_v4_count_list.max() \n",
    "print(\"Min {} and Max {} \".format(np.round(v_min, 4), v_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_min = np.where(tfidf_v4_count_list == v_min)\n",
    "idx_max = np.where(tfidf_v4_count_list == v_max)\n",
    "\n",
    "print(\"Min feature(s): \", tfidf_v4_vocab[idx_min])\n",
    "print(\"\\nMax feature(s): \", tfidf_v4_vocab[idx_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v4_fit.max(axis=0).toarray(), tfidf_v4_fit.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find maximum value for each of the features over dataset:\n",
    "vals_max = tfidf_v4_fit.max(axis=0).toarray().ravel() #class numpy.ndarray\n",
    "print(\"Max value shape:\", vals_max.shape)\n",
    "\n",
    "sorted_by_tfidf = vals_max.argsort()\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(tfidf_v4_vocab[sorted_by_tfidf[:20]]))\n",
    "\n",
    "print(\"Features with highest tfidf: \\n{}\".format(tfidf_v4_vocab[sorted_by_tfidf[-20:]]))\n",
    "\n",
    "sorted_by_idf = np.argsort(tfidf_v4.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(tfidf_v4_vocab[sorted_by_idf[:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EDA Final Vectorizer WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$actodo https://stackoverflow.com/questions/53997443/how-to-add-extra-stop-words-in-addition-to-default-stopwords-in-wordcloud\n",
    "my_stop_words = list(uds_stopwords) + list(text.ENGLISH_STOP_WORDS)\n",
    "len(my_stop_words), len(uds_stopwords), len(text.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(stopwords = my_stop_words, background_color=\"white\", max_words=5000, include_numbers=True, collocation_threshold=5, width=600, height=300, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords = my_stop_words, background_color=\"white\", width=800, height=400,).generate(long_string) #\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  NOT Join tokens together.\n",
    "#long_string = ','.join(list(text_train))\n",
    "long_string = ','.join(tfidf_v4.vocabulary_)\n",
    "len(long_string)\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(stopwords = \"english\", background_color=\"white\", max_words=5000, include_numbers=True, collocation_threshold=5, width=600, height=300, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xtra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python - sum of all counts in collections.Counter\n",
    "# refer to https://testdriven.io/tips/6729e7af-9482-4b37-a780-fab42b709841/\n",
    "from collections import Counter\n",
    "\n",
    "pencil_stock = Counter({\"Red\": 17, \"Blue\": 5, \"Green\": 9}) #Counter for dicts\n",
    "\n",
    "print(len(pencil_stock), pencil_stock.total())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xtra\n",
    "\n",
    "#### Bag-of-Words\n",
    "- Tokenize dataset and build the vocablulary  \n",
    "- Revew vocabulary and features  \n",
    "- Create bow representation of training data - SciPy sparse matrix\n",
    "- dtm \"dense\" NumPy array to look at the actual content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by default extracts tokens using a regex \"\\b\\w\\w+\\b\" - all sequences of chars that consist of at least two letters or numbers (\\w) and that are separated by word boundaries(\\b) => no single letter words, splits `don't` or `bit.ly`\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "# `fit` to tokenize train data and build the vocabulary - <class 'dict'> {word,index}\n",
    "vectorizer.fit(text_train.iloc[:10])\n",
    "\n",
    "# access vocab with attribute vocabulary_ <class 'dict'>\n",
    "print(\"Vocab size: {}\".format(len(vectorizer.vocabulary_)))\n",
    "print(\"Vocab (with word indices): {}\".format(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features\n",
    "print(\"Vocab size:{}\".format(len(vectorizer.get_feature_names())))\n",
    "print(\"Vocab (ordered alphabetically): {}\".format((vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `transform` to convert text to a bag of words\n",
    "bow = vectorizer.transform(text_train.iloc[:10]) \n",
    "#SciPy sparse matrix only stores non-zero entries\n",
    "\n",
    "print(\"Bag-of-words: {}\".format(repr(bow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer uses a sparse array to save memory\n",
    "# convert to a \"dense\" NumPy array to look actual content\n",
    "dtm = bow.toarray()\n",
    "\n",
    "print (\"document-term dimensions:\", dtm.shape)\n",
    "print (dtm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually get word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 records x 121 words\n",
    "print (dtm.shape)\n",
    "\n",
    "#each row=record, how many words each?\n",
    "print(np.sum(dtm, axis=1))\n",
    "\n",
    "#each column=word, frequency of each word?\n",
    "print(np.sum(dtm, axis=0))\n",
    "\n",
    "#confirm sums\n",
    "np.sum(dtm, axis=1).sum(), np.sum(dtm, axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$xtra save vocab base \n",
    "csv_columns = ['word','idx']\n",
    "csv_file = \"myExplore/vocab_ngrams3_index.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as f:\n",
    "        for key in vect_ngram.vocabulary_.keys():\n",
    "            f.write(\"%s,%s\\n\"%(key,vect_ngram.vocabulary_[key]))\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toGist $actodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save vocab c1 \n",
    "csv_columns = ['word','idx']\n",
    "csv_file = \"myExplore/vocab_base_index_tfidif_min001.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as f:\n",
    "        for key in vect_c2.vocabulary_.keys():\n",
    "            f.write(\"%s,%s\\n\"%(key,vect_c2.vocabulary_[key]))\n",
    "except IOError:\n",
    "    print(\"I/O error\")\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.544px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
